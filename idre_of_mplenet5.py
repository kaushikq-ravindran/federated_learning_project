# -*- coding: utf-8 -*-
"""idre of MPLENEt5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Krc4new_xptKYqM374Xs20mfslYXVjEU
"""

import pandas as pd
import numpy as np
#data visualization packages
import matplotlib.pyplot as plt
#keras packages
import keras
from keras.models import Sequential
from keras.layers import Convolution2D
from keras.layers import MaxPooling2D
from keras.layers import Flatten
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.layers import Dropout
#model evaluation packages
from sklearn.metrics import f1_score, roc_auc_score, log_loss
from sklearn.model_selection import cross_val_score, cross_validate

mnist = keras.datasets.mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

x = X_train.reshape(60000, 28*28)
df = pd.DataFrame(x)
df['labels'] = y_train
df.head()
df = df.sample(frac=1)

out1=df[:100]
out2=df[100:200]
out3=df[200:300]

dfr = df[300:]
dfr.head()
dfr = dfr.sample(frac=1)

data2

dfr = dfr.sort_values(by ='labels' )

dfr = dfr[dfr['labels']<3]

data1 = df[:20000]

data2 = df[20000:40000]

data2 = dfr[18532:35840]
data2 = data2.append(out2)

data3 = df[40000:]

data3

X1_train = data1.drop(['labels'],axis=1).values.reshape(20000, 28, 28)
y1_train = data1['labels'].values

X2_train = data2.drop(['labels'],axis=1).values.reshape(20000, 28, 28)
y2_train = data2['labels'].values

X3_train = data3.drop(['labels'],axis=1).values.reshape(20000, 28, 28)
y3_train = data3['labels'].values

X_train = data.drop(['labels'],axis=1).values.reshape(30596, 28, 28)
y_train = data['labels'].values
print(X_train.shape, y_train.shape)

from sklearn.model_selection import train_test_split
X1_train,X1_ttest,y1_train,y1_ttest=train_test_split(X1_train,y1_train,test_size=0.1667,shuffle =True ,random_state = 2021)

from sklearn.model_selection import train_test_split
X2_train,X2_ttest,y2_train,y2_ttest=train_test_split(X2_train,y2_train,test_size=0.1667,shuffle =True ,random_state = 2021)

from sklearn.model_selection import train_test_split
X3_train,X3_ttest,y3_train,y3_ttest=train_test_split(X3_train,y3_train,test_size=0.1667,shuffle =True ,random_state = 2021)

from sklearn.model_selection import train_test_split

X1_train, X1_val, y1_train, y1_val = train_test_split(X1_train, y1_train, test_size=0.1667, shuffle=True, random_state=2021)

X2_train, X2_val, y2_train, y2_val = train_test_split(X2_train, y2_train, test_size=0.1667, shuffle=True, random_state=2021)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1667, shuffle=True, random_state=2021)

X3_train, X3_val, y3_train, y3_val = train_test_split(X3_train, y3_train, test_size=0.1667, shuffle=True, random_state=2021)

import tensorflow as tf
X_train = X_train / 255.0
X_val = X_val / 255.0

X_train = tf.expand_dims(X_train, 3)
X_val = tf.expand_dims(X_val, 3)

import tensorflow as tf
X1_train = X1_train / 255.0
X1_val = X1_val / 255.0

X1_train = tf.expand_dims(X1_train, 3)
X1_val = tf.expand_dims(X1_val, 3)

import tensorflow as tf
X2_train = X2_train / 255.0
X2_val = X2_val / 255.0

X2_train = tf.expand_dims(X2_train, 3)
X2_val = tf.expand_dims(X2_val, 3)

import tensorflow as tf
X3_train = X3_train / 255.0
X3_val = X3_val / 255.0

X3_train = tf.expand_dims(X3_train, 3)
X3_val = tf.expand_dims(X3_val, 3)

X1_ttest = X1_ttest / 255.0
X1_ttest = tf.expand_dims(X1_ttest, 3)

X2_ttest = X2_ttest / 255.0
X2_ttest = tf.expand_dims(X2_ttest, 3)

X3_ttest = X3_ttest / 255.0
X3_ttest = tf.expand_dims(X3_ttest, 3)

X_test = X_test / 255.0
X_test = tf.expand_dims(X_test, 3)

X_train[0].shape

lenet_5_model = keras.models.Sequential([
    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same'), #C1
    keras.layers.MaxPooling2D(), #S2
    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C3
    keras.layers.MaxPooling2D(), #S4
    keras.layers.Flatten(), #Flatten
    keras.layers.Dense(120, activation='tanh'), #C5
    keras.layers.Dense(84, activation='tanh'), #F6
    keras.layers.Dense(10, activation='softmax') #Output layer
])

lenet_5_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
lenet_5_model.summary()

from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='val_loss',patience = 7,min_delta=0,verbose=0,mode='auto', baseline=None, restore_best_weights=True)
history =lenet_5_model.fit(X_train, y_train, epochs=500,validation_data=(X_val, y_val),steps_per_epoch=100,callbacks = [es])

score = lenet_5_model.evaluate(X_test, y_test, verbose=1)
print('Test Loss:', score[0])
print('Test accuracy:', score[1])

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

lenet_5_model.save('model12.h5')

del lenet_5_model

from keras.models import load_model
model4 = load_model('model12.h5')



score = loaded_1.evaluate(X_test, y_test, verbose=1)
print('Test Loss:', score[0])
print('Test accuracy:', score[1])

time = [0, 1, 2, 3,4]
iid3 = [0, 98.54999780654907, 98.79999756813049, 98.86999726295471,98.90999794006348]
iid4 =[0,98.36000204086304,98.82000088691711,98.89000058174133,98.89000058174133]
iid5 =[0,98.15999865531921,98.60000014305115,98.79000186920166,98.76000285148621]
noniid2k = [0,95.09000182151794,95.56999802589417,96.53000235557556,96.60000205039978,
96.65999707221985]
noniid1k= [0,92.6599979400634,94.4100022315979,94.47000026702881,94.33000087738037]
noniid500= [0,88.08000087738037,88.37000131607056,92.44999885559082,92.36000180244446]
noniid100= [0,67.9099977016449,68.12000274658203,69.7700023651123,73.72999787330627,75.55000185966492]
set(gca,'ytick',ymin:.1:ymax)
plt.plot(time, position1)
#plt.plot(time,position2)
plt.legend(["4 Participants", "5 Participants"])
plt.ylabel('Accuracy')
plt.xlabel('Stages')

import matplotlib.ticker as ticker
time = [ 0,1, 2,3,4,10]
iid3 = [0.94,.9854999780654907, .9879999756813049, .9886999726295471,.9890999794006348,.9890999794006348]
iid4 =[0.94,.9836000204086304,.9882000088691711,.9889000058174133,.9889000058174133,.9890999794006348]
iid5 =[0.94,.9815999865531921,.9860000014305115,.9879000186920166,.9876000285148621,.9880999794006348]

#plt.plot(time,iid4)
#plt.plot(time,iid3)
plt.plot(time,iid5)
plt.legend(["5Participants", "3 Participants","5 participants"])

time = [0,1,2,3,4,5]
noniid2k = [0.65,.9509000182151794,.9556999802589417,.9653000235557556,.9660000205039978,
.9665999707221985]
noniid1k= [0.65,.926599979400634,.944100022315979,.9447000026702881,.9433000087738037,.9433000087738040]
noniid500= [0.65,.8808000087738037,.8837000131607056,.9244999885559082,.9236000180244446,.9236000180244450]
noniid100= [0.65,.679099977016449,.6812000274658203,.697700023651123,.7372999787330627,.7555000185966492]
#plt.plot(time,noniid2k)
#plt.plot(time,noniid1k)
#plt.plot(time,noniid500)
plt.plot(time,noniid100)
plt.legend(["100", "1k","500","100"])

superdataset=[0.65,.988099992275238,.9898999929428101,.9886000156402588,.9900000095367432,.9901999831199646]
BDML = [0.65,.9855999946594238,.9873999953269958,.9876000285148621,.9851999878883362,.9872000217437744]
time = [0,30,50,100,200,500]
plt.plot(time,superdataset)
plt.plot(time,BDML)
plt.legend(["Superdataset","BDML"])